# 统计图形 {#statistical_graph}

- 直方图、条形图和饼图

  一般来说，直方图用于描述连续随机变量的分布状况，而条形图用于刻画离散随机变量的分布状况。饼图有一个全局的概念，在表达比例的时候比较常用。

- 两个连续变量之间的相关关系

  关于两个连续变量之间的相关关系，可用散点图、回归分析和相关分析方法进行分析。

   - 相关分析
   
   皮尔逊相关系数主要用于定距变量，斯皮尔曼相关系数主要用于定序变量，而肯德尔相关系数主要用于定类变量。
   
    - 皮尔逊相关系数（正态分布）
       
    $$p_{xy} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i  - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}$$

    - 斯皮尔曼相关系数
     
     对原始的 $x_i, y_i$ 从大到小排序，记 $x_i^{'}, y_i^{'}$ 在原始排列中的位置，称 $x_i^{'}, y_i^{'}$ 为秩，$d_i = x_i^{'} - y_i^{'}$ 为秩差，则 Spearman 秩相关系数为：$$1 - \frac{6* \sum\limits_{i=1}^n d^2}{n (n^2 - 1)}$$
     
     如果有重复数据，在计算秩时，取均值。
     
     - 肯德尔相关系数
     
     $$\tau_a = \frac{C - D}{\frac{1}{2} N (N -1)}$$
     
     其中 C 表示 XY 中拥有一致性的元素对数（两个元素为一对）；D 表示 XY 中拥有不一致性的元素对数。此公式仅适用于集合X与Y中均不存在相同元素的情况。
     
     
     $$\tau_b = \frac{C -D}{\sqrt{(N_3 - N_1)(N_3 - N_2)}}$$
     
     其中 $N_3 = \frac{1}{2}N(N-1)$, $N_1 = \sum\limits_{i=1}^s \frac{1}{2} U_i (U_i - 1)$,$N_2 = \sum\limits_{i=1}^t \frac{1}{2} V_i (V_i - 1)$.
     
     将 $X$ 中的相同元素分别组合成小集合，$s$ 表示集合 $X$ 中拥有的小集合数（例如X包含元素：1 2 3 4 3 3 2，那么这里得到的 $s$  则为2，因为只有2、3有相同元素），$U_i$ 表示第 $i$ 个小集合所包含的元素数。$N_2$ 在集合 $Y$ 的基础上计算而得。
     
     
- 箱线图

- 方差分析

  它是比较两个或多个因素效应大小，它的自变量为示性变量，表示某种效应的存在与否。

  
- Causs-Markov 假设

  - $E(\epsilon) = 0$，这表明 $y_i$ 在其均值 $\bar{y}$ 附近  的波动完全是随机性的。 
  
  - $cov(\epsilon_i, \epsilon_j) = 0$，这表明不同次的观测值是不相关的。
  
  - $var(\epsilon_i) = \sigma^2$，这表明 $y_i$ 在其均值附近 $\bar{y}$ 附近的波动程度是一样。
  
估计回归参数最基本的方法是最小二乘法。

- Causs-Markov 定理表明对于线性回归模型中，在所有的线性无偏估计中，最小二乘估计是唯一具有最小方差的估计。

$$RSS = Y^{'}Y - \hat{\beta}X^{'}Y$$

    $RSS$，它的大小反映了实际数据与理论模型的偏离程度或者拟合程度。
    
    $R^2$，判定系数度量的是自变量对因变量的拟合程度，该值越大，表明自变量与因变量有较大的相依关系。
    
- 多重共线性
 
  回归自变量之间有近似的线性关系，即 $c_1 x_1 + \cdots + c_p x_p = 0$.
  
  度量复共线性严重程度的一个重要的统计量为 $k = \lambda_1 /\lambda p$.
  
  在线性回归中检测多重共线性，可以使用方差膨胀因子。
  
- 异常值

  在统计学中，异常值是泛指在一组数据中，与它们主体不是来自同一个分布的那些少数的点。
 
- 相关系数

  两个变量之间的关系可以分为独立和相依，在相依中又分为线性相依和非线性相依，相关系数度量的是线性相依关系。
  
  独立一定是不相关， 但是不相关不一定是独立。
  
  
- 大数定理

  随机变量的均值渐进趋近于一个常数。

- 中心极限定理

  表明随机变量的和渐进分布为正态分布。
  
- 中位数和均值

  - 中位数存在，但均值不一定存在。
  
  - 高阶矩存在，低阶矩一定存在，但是低阶矩存在，高阶矩不一定存在。
  
  - 均值容易受到异常值的影响，但是中位数不易受到异常值的影响。
  
  - 中位数
  
  将随机变量的取值从小到大排序，中位数为排序中间的值。表示，大体上，比中位数大或者小的数据的个数为整个数据的一半。
  
  
- 极差和半极差

  极差和半极差都是用来度量数据的分散性，半极差对于异常值比较稳健。


- 经验分布
   
  对于连续性的随机变量可以用直方图来描述整体的分布情况。对于一般的随机变量，可以用经验分布函数来描述随机变量的分布情况。
  
  
  **经验分布函数**的定义如下：

  设 $X_1, \cdots, X_n$ 为抽自总体 $X$ 的样本, $X \sim F(x)$, 则称 $$F_n(x) = \frac{1}{n} K(x)$$, 其中 $K(x)$ 表示不小于 $x$ 的个数。
  
  **QQ 图**
  
  横坐标为分位数，纵坐标为随机变量的取值，如果 QQ 图是一条直线，那么检验可知随机变量服从正态分布。
  
  
- 正态性检验

- 分布拟合检验

- 参数估计

  - 矩估计
  
    - 思想
     
      样本矩估计总体矩
  
  - 极大似然估计
  
  对于极大似然估计需要知道总体分布形式，计算相对复杂。
  
  - 估计量优良性准则
  
    - 相合性
    
    - 无偏性
    
    - 有效性
    
  树的模型对于预测变量不敏感，预测变量需要变化主要有一下几个方面：
  
  - 建模方法对预测变量有严格的要求
    
    预测变量有相同的标度
  
  - 数据的某些特征可能导致建模的困难
  
    判断数据有偏的黄金标准，如果最大值与最小值的比例超过20，认为该数据显著有偏。
    
    另外可以用偏度进行描述，
    
    $$偏度 = \frac{\sum (x - \bar{x})^3}{(n-1) v^{\frac{3}{2}}}$$
    
    其中，$v = \frac{\sum(x_i - \bar{x})^2}{n-1}$
    
    
  - 数据降维和特征提取
  
  PAC, 寻找若干原始预测变量的线性组合，它们捕捉最多预测变量的组合。之后的主成分和之前的主成分不相关。
  
  在进行主成分之前先判断是否有偏，对有偏的变量进行变换，然后对变量进行标准化和中心化。
  
  - K 折交叉验证
  
  - Boostrap 方法
  
  放回随机抽样，一些样本可能被抽取到多次， 但是一些样本一次也没有被抽取，这些一次也没有抽取到的样本，称为袋外样本，选中的样本被用于训练和建立模型，袋外样本被用于测试模型。
  
  - 632 法
  
  $$0.632*简单 Boostrap 估计错误率 + 0.368*显性错误率$$
  
  - 模型效果的定量度量
  
    - 均方根误差
    
      解释为观察值与预测值之间的平均距离。
      
    - 决定系数
    
      数据包含的信息中能被模型所解释的比例。
      
    - MSE
    
      MSE = $\sigma^2$ + 偏差的平方 + 方差
      
  - 线性回归
  
    高度可解释性
    
    $$\hat{\beta} = (X^TX)^{-1}X^{T}y$$
    
    $X^TX$ 可逆的条件：
    
       1）没有任何一个变量是其它变量的线性组合

       2）样本量的个数大于预测变量的个数。
       
    多元线性回归的第三个显著问题在于，它容易受那些远离主要数据总体趋势的点的影响。 
    
    - 岭回归
    
    - Lasso 回归
    
    - 弹性网
    
    稳健回归目标是在回归模型中最小化异常值的影响。
    
  - KNN 
    
  - SVM
    
    基本思想：寻求能最大程度将训练数据分开的超平面，对于线性可分的训练数据集，这样的超平面有无群多个，需要找出最佳的分离超平面。最大程度不仅能够将正负样本划分开来，而且能够将距离超平面最近的样本以足够大的确信成都区分开来。`
    
    如何衡量最佳？
    
    - 一个是将训练数据点分类正确，给正负label各自划分到各自的类别中去；
    
    - 不仅将各自的label划分正确，而且还能够给出他们各自划分到各自的类别中的确信程度尽可能地大。
    
    为了满足上面的两个条件，引进了函数间隔，即 $y(wx+b)$,具体表现如下：
    
    - $y(wx+b)$ 符号尽可能一致，描述的是上述条件1；
    
    - $|wx+b|$ 的距离尽可能地大，描述的是上述条件2。
  
      函数间隔与几何间隔
  
      训练数据中与分离超平面最近的实例称为支持向量。
      
    - 线性可分
    
    $$min_{w,b}\frac{1}{2}||w||^2 $$
    
    
    $$s.t. y_i(wx_i + b) - 1\geq 0 $$
    
    - 线性不可分（松弛变量）
    
    - 非线性可分
    
      核技巧，通过非线性变化将输入空间对应于特征空间
      
  - Boosting
  
    强可学习：一个类中， 如果存在多项式  算法学习，并且学习的准确率较高，那么称这个算法为强学习器。反之，如果学习的准确率仅仅比随机猜测略好，那么这个算法称为弱学习器。
    
  - adaBoost 
  
    - 每一轮如何改变训练样本的权重
      
      提高上一轮训练错误的赝本的权重，降低训练正确样本的权重。
    
    - 如何将弱分类器组织成强分类器
    
      加权多数表决，加大分类错误率小的弱分类器的权重，使其在表决中起较大的作用。
      
  - 决策树
  
    结构：节点和有向边，节点又分为内部节点和叶子节点
  
    过程：特征选择、决策树生成、剪枝 
    
    决策树模型的复杂度有叶子节点的个数以及以及叶子节点对应的 $L^2$ 范数决定。
      
    - ID3
   
      在决策树各个节点上面基于信息增益选择特征， 递归地构建决策树。选择信息增益最大的特征以及划分阈值。
      
    - G4.5
    
      穷举每一个 feature 的每一个阈值，找到使得 $feature \geq  x , feature < x$ 分成的两个分支的熵的 feature 以及阈值 $x$,直到所有的样本有唯一的叶子节点对应或者满足终止条件为止。
      
  
    - CART
  
      回归树用平方误差最小化构建二叉树，对分类树用基尼指数最小化准则。
  
  
  - GBDT
  
  - 随机森林
  
    通过自助法重采样技术，从原始训练样本 $N$ 中有放回地重复抽样 $k$ 个样本生成新的训练集样本集合，然后根据自助采样集生成 $k$ 个决策树生成的随机森林。
    
    以决策树作为基学习器，另外在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性时在当前节点属性集合中选择一个最优属性；而随机森林对每一个决策树的每一个节点，先从该节点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后从这个子集中选择一个最优属性用于划分。
    
  - 位置参数和形状参数
  
    位置参数描述的是集中趋势，如均值和中位数。
  
    形状参数，确定分布函数的形状，正态分布中的标准差。
  
  
  

  
    
    
    
       
   
   
   
